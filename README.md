# Decentralized IDentifiers (DID) protocol

Communicating beween DWNs is done by the `did://` protocol, which tries to find
the address of a DWN by its DID. This protocol works over TCP/IP.

A DID address is essentially a public key derived from a master key. The private
key of a DID is used for signing, and the public key for network identification
and proofing. Therefore you should distinguish the `PDID` (private DID, or the
private key) and the DID (public key).

> The `PDID` concept will only be used in this document.

## How is it used?

`did://` URLs are formatted as `did://<address>/` or `did://<dns>:<common_name>/`
. This difference allows for some DWNs to be targeted using common names (such
as `did://rift:google`) without any issues. The common name method implies 
however the existence of a federated DNS record to store without altering the
common name's DWN reference. The `dns` part of a common name refers to the DNS
service to use:
- `rift` is not a valid DNS service!
- An **Address Record** (AR) is DNS-like table where neighbors are stored. It
    is explained in more details below.
- A DNS address syntax has to be either <domain_name> or <domain_name>:<tld>.
    If the domain name is under the `.com` TLD, services will automatically look
    for them.
- CommonRift Web5's DNS holds a AR of every DWN that connected to them and
    regularly tries to contact them to keep the AR up-to-date.
- CommonRift Web5's DNS is explained in depth on its repo (`@riftworks/did-dns`)

> `did://somedns:io:github` is a valid DNS DID address
> `did://github` is not a valid DNS DID address

### Valid DNS DID addresses (16/06/2025)

| DID DNS Name      | HTTP Address          | Owner                           |
| ----------------- | -----------------     | ------------------------------- |
| commonrift        | commonrift.com        | Org owner                       |
| johanmontorfano   | johanmontorfano.com   | Org owner                       |

Multiple requests verbs exists on the DID protocol:
- `PREFLIGHT` with `did://<address>` to verify the identity of a DID 
- `WHERE` with `did://<address>?` for target lookup
- `WHERE` with `did://<address>!` for target with storage lookup
- `#DATA` with `did://<address>` for data exchange with the target DID handler
> [!NOTE]
> This verb is important to enable the DID handler to answer vital requests 
> for the network that does not constitute vital actions and relates to its
> state.
- `DATA` with `did://<address>` for target data exchange

## How it works?

This library enables `did://` servers based on the TCP/IP stack. The library
opens a socket connection to a port, and read/write with it.

The protocol works over TCP/IP and provides similar functionnalities as HTTP.

Each DID server is responsible for creating a session ECDHE key for each 
incoming connection (session). The use of `ECDHE` keys is described in the
`PREFLIGHT` process below.

Requests are composed in a identity part and a body part.

### The identity part

The identity part of a request is about the sender identity, identity proofs,
and request verb.

Each sane `did://` library should verify the target holds the correct did and
is not spoofing anyone.

From [DWN's README](https://github.com/riftworks/dwn/README.md)

> ### Anti-spoofing verifications
>
> Verifying a DWN holds the correct identity is the painpoint of naive systems.
> Doing a simple challenge by verifying the DWN holds the correct key-pair is not
> enough as it does not prove it is meant to hold a given DID. Therefore, any DWN 
> is able to pretend it is honest by simply generating an appropriate key-pair.
> 
> To prevent spoofing, DWNs will be using ZK-proofs of knowledge and every key
> and the DID are generated by the DWN from a master key. Using a master key
> allows for every DWN to prove a DWN is holding the correct DID without exposing 
> any sensitive data.
> 
> It is done by challenging a DWN to prove it can derive the correct key for a
> specific epoch. The DWN will have to give this key and a proof of knowledge of
> the master key. DWNs will then be able, using Schnorr-like proofs, to determine
> if the derived key has the same master key as the DID. The verification flow is
> the following:
> 
> - Challenger: "Prove you can derive the signing key for epoch N"  
> - DWN: Provides derived key + Schnorr-like proof of master key knowledge
> - Challenger: Verifies proof confirms same master key generated both DID and
>   derived key
> 
> In résumé:
> - **Master Key Architecture**: each DWN generates a master key that never leaves
>    the node
> - **Deterministic Derivation**: both the DID and all operational keys derive
>     from this master key
> - **ZK Proof of Knowledge**: the DWN proves knowledge of the master key without
>     revealing it
> - **Epoch-Based Verification**: verifiers can request proof of key derivation
>     for any epoch


Therefore, the identity part is important to verify the authenticity of the
target. Before sending a request for the first time to a target (no active
session), a `PREFLIGHT` request is sent to initiate the `PREFLIGHT` process.

Every request is formatted as, with commas as separators for identites and
two new lines to separate the identity from the body.

```
<VERB>,<URL>,<DID>,<IP>,<SIZE>

<BODY>
```

The header contains the verb, the url, the sender DID, the sender IP and the
size. The size property is used to notify the receiver of the size of the
request since TCP/IP does not have a way of notifying for the end of a request
since it's... a stream.

### `PREFLIGHT`

The `PREFLIGHT` process is done in multiple phases:
1. The server creates an ephemeral ECDHE pair for this connection's session
2. The server sends a reduced header and its ECDHE public key with the signature
    made with its PDID
```
<VERB>,<DID>,<IP>,<SIZE>

ECDH_ONLY
<ECDH_SESSION_PUBLIC_KEY>
<ECDH_SESSION_PUBLIC_KEY_SIGNED>    // Signed with the PDID
```
3. The target creates an ephemeral ECDHE pair for this connection's session and
    verifies the ECDH key has not been tampered by using the DID. If the 
    verification fails, the connection is closed
4. The target sends a reduced header and its ECDHE public key
5. The server verifies it the ECDH key of the target has not been tampered. If 
    the verification fails, the connection is closed
6. The server and the target computes the secret from each other public key and
    their respective private key using `Secp256k1` to generate a AES-256 key 
    from this secret. From now on, all the content of requests is encrypted and
    headers are full
7. The server sends a request to let the target verify its DID identity with a 
    part of its AR table for the sake of neighboring good practices.
```
<VERB>,<URL>,<DID>,<IP>,<SIZE>

DID_PROOF
<ZK_PROOF_OF_MASTER_KEY>
<CHALLENGE_NONCE>
<AR_TABLE_SLICE>
```
8. The target verifies the ZK-proof with the server's DID to determine if both
    values are derived from the same master key, and the target sends the same
    content to let the server verify its DID. If the verification of DID failed
    the connection is closed.
9. The server verifies the target's DID and closes the connection if it failed.
10. At this point if the connection is not closed, the server has to send its
    original request. If this `PREFLIGHT` request was just meant to verify
    neighbors availability, the server must send the following content to the
    target which will close the connection.
```
<VERB>,<URL>,<DID>,<IP>,<SIZE>

NEIGHBORING_ONLY
```

> [!IMPORTANT]
> During the preflight process, ECDHE remote public key is used with the local
> ECDHE private key with a `Secp256k1` elliptic curve to get a secret. This 
> secret is then used to create a AES-256 derived key for symmetric key 
> encryption.
>
> Due to the nature of the Diffie-Hellman algorithm, the ECDH calculation made
> on public and private keys will produce the exact same output. Ensuring both
> AES keys are the same without ever sharing them.
>
> All signatures during this process are done using ECDSA and the same curve as
> ECDHE keys.

> [!NOTE]
> After the `PREFLIGHT` handshake, all requests and responses are fully 
> encrypted.

**This way, there is no reason to reauthenticate DIDs for procedures, since 
they are always authenticated.**

### The body part

The body part can be any UTF-8 content.

## Discovery with `did://`

The default behavior of every DID device is to gather a list of neighbors as 
quickly as possible to rotate with available neighbors as often as possible.

When a specific DID is targeted, and is not present in the local AR table, 
it will ask each of its neighbor a `WHERE` request formatted as 
`did://<address>?` with a JSON body.

```json
{
    "created_at": "timestamp",
    "request_id": "uuid",
    "requested_by": "DWN_REQUESTER",
    "requested_by_ip": "DWN_REQUESTER_IP",
    "depth": "SIZE_OF_THE_NEIGHBOR_TREE_SINCE_REQUEST"
}
```

Each neighbor will replicate this request to their neighbors until one neighbor
returns the IP address of the looked-up DWN. The query is aborted under those
conditions:
- `"depth" > 30`.
- Older than 30 seconds.
- No more nodes that did not answer the request can be reached.

The duplication of `WHERE` requests is a great way for DWNs to register new
neighbors.

The response to a `WHERE` request is formatted as:

```json
{
    "request_id": "uuid",
    "from_dwn": "DWN_REQUESTER",
    "from_dwn_ip": "DWN_REQUESTER_IP",
    "requested_address_found": "bool",
    "requested_address": "<address>",
    "requested_address_ip": "<address>_IP"
}
```

> Therefore, connecting to a DNS-namespaced DID target is a great way to gather 
> a vast amount of neighbors fast.

### Neighboring

When a lookup request is received by a DID server, the asker is reached to be 
added to the AR table if one of those conditions is valid:
- Not enough neighbors
- Too much unalive neighbors
- Room available on the AR table

`PREFLIGHT` requests are sent to neighbors to check if they are still alive and
if they have a valid identity. If a neighbor does not have a valid identity, it
is removed from the AR table.

#### Neighbors Scoring

In a distributed system with no central authority, malicious nodes taking 
advantage of the network can happen. Those nodes poses a real security threat 
to the integrity of the network, especially for their neighbors.

To avoid running in conditions where malicious nodes take down an entire system,
each DID keep a list of positive and negative events, which can be made into a
score.

| Event name                                        | Score                   |
| ------------------------------------------------- | ----------------------- |
| Penalty for bad average scoring by neighbors      |                   0..-5 |
| PREFLIGHT cryptographic handshake error           |                      -2 |
| Unexpected remote TCP/IP connection termination   |                      -1 |
| Unavailability as storage provider                |                     -.5 |
| Unavailability as neighbor                        |                   -.005 |
| Did not paid for storage                          |      -.02/Go * replicas |
| Being a storage provider                          |       +.2/Go * replicas |
| Lookup participation as neighbor                  |                       1 |
| Good average scoring by neighbors                 |                    0..5 |

Each node retains only the last 40 events, and each event expires after 21 days
to encourage active participation, and to not provoke network fragmentations 
with bad-rated nodes.

Scores are stored in a table as:

| Event name                                        | Timestamp               |
| ------------------------------------------------- | ----------------------- |
| Some event                                        | Some timestamp          |

A scoring table cleanup is performed everytime the table is accessed.

Some thresolds applies regarding scoring:
- **Below -15**, the bad scorer is considered in quarantine by the node.
    - Lookup participation is allowed.
    - Storing or being a storage provider is disallowed.
    - The node will stop using it as a storage provider.
- **Below -5**, the bad scorer is not trusted anymore for AR tables exchange.

> [!NOTE]
> No nodes gets high-scoring advantages because trust is not a reward system 
> but rather a shield against suspicious activities. It keeps the whole network
> away from favoritism or nodes gaming for benefits.

DIDs can be targeted at `#DATA did://<address>/ar/score/<did>` to retrieve the
scoring event list of a DID regarding another DID.

Response body:
```jsonc
{
    "for_target": "did",
    "scoring_log": [
        {
            "event_name": "event_name",
            "timestamp": "timestamp"
        },
        // etc...
    ]
}
```

### The AR table

Making everything more efficient is highly required for such a system.
Therefore, each DID-enabled device should feature an address record that stores
the IP address of every DID instance running.

This record is formatted as:

| DID               | IP                | Last Checked      | Alive           |
| ----------------- | ----------------- | ----------------- | --------------- |
| `abcdefghij...`   | `12.12.12.12`     | `some epoch`      | `true`          |
| ...               | ...               | ...               | ...             |

It also features infos on the storage of the device:

| Avail. Storage    | Data Consistency  | Avg Network Speed | Current Price   | Availability | Replicated On | Stored Data For   |
| ----------------- | ----------------- | ----------------- | --------------- | ------------ | ------------- | ----------------- |
| `12Go`            | `.99`             | `12Mo/s`          | `0.000007btc/Go`| `.98`        | `[did, did]`  | `[did, did, did]` |

The DID library can be configured to properly size this table, but setting it
to a small size may cause the device to be disconnected from the network.

To avoid as much as possible this case, the protocol will regularly ping its
neighbors to determine if they are still alive. If the neighbors number is low,
the DID will try to get new neighbors from unalive connections.

#### DNS DID

The aforementioned DNS DID system allows for entities to establish large AR
tables with "domain names" to let DID servers connect to a specific DID 
with a common name. Therefore, `did://dns:google` would redirect you to the
Google search page hosted on a DID (even if it makes no sense in such a system)

DNS DIDs can also play a big role in creating large federations of trusted DNS
to allow DID clients to have a small AR table and rely more on multiple DNS DIDs
to find DIDs.

As mentioned before, DNS DIDs are servers targeted via the HTTPS protocol. 
However, the HTTPS handshake only allows to retrieve their IP. Right after the
connection, DID servers will switch to a DID connection with an initial 
`PREFLIGHT`.

> [!NOTE]
> The goal is to setup, at some point, trusted DNS with native DID that remains 
> widely available and accessible directly by IP addresses. Like what we see 
> today for HTTP-enabled devices to get access to the internet.

#### Requesting a slice of an AR table

Any DID can request a slice of another DID's AR table. This request is done 
through the `#DATA` verb, target `did://<address>/ar/get`, and the body should
be formatted as:

```json
{
    "get_entries": 26
}
```

The answer body should be formatted as:

```json
[
    {
        "did": "did",
        "ip": "ip",
        "last_checked": "timestamp",
        "alive": "boolean",
        "available_storage": "12Go",
        "availability": .99,
        "data_consistency": .99,
        "avg_network_speed": "12Mo/s",
        "current_price": "0.000000007btc/Go"
    },
    ...
]
```

#### AR table size recommendations

- The AR table size baseline should be 10 active neighbors.
- A big AR table size is >500 active neighbors.
- A DID DNS AR table size should be >1000 active neighbors.

## Storage lookup

Storage lookup is slightly different from basic lookup because we query the
network for storage characteristics and not for an IP. Therefore, the content
of a `WHERE!` (storage lookup) request is slightly different than a `WHERE?`
response.

```jsonc
{
    "created_at": "timestamp",
    "filter_by": {
        "min_availability": .92,
        "min_data_consistency": .8,
        "min_available_storage": "20Go",
        "min_avg_network_speed": "10Mb/s",
        // Can be used to get providers with a minimum price per unit
        "min_price": "0.00000000001btc/Go",
        // Can be used to not get providers past this price per unit
        "max_price": "0.0000000005btc/Go",
        // Number of storage providers this provider uses for data replication
        "min_replications": 3,
        // Number of past clients (either for replication or main nodes)
        "stored_data_for_at_least": 5
    },
    "requested_by": "DWN_REQUESTER",
    "requested_by_ip": "DWN_REQUESTER_IP",
    "depth": "SIZE_OF_THE_NEIGHBOR_TREE_SINCE_REQUEST"
}
```

Each `filter_by` field is optional.

The response is different too:

```json
{
    "from_dwn": "DWN_REQUESTER",
    "from_dwn_ip": "DWN_REQUESTER_IP",
    "filters": {
        "min_availability": .92,
        "min_data_consistency": .9,
        "min_available_storage": "20Go",
        "min_avg_network_speed": "10Mb/s",
        "min_price": "0.00000000001btc/Go",
        "max_price": "0.0000000005btc/Go",
        "min_replications": 3,
        "stored_data_for_at_least": 5
    }, 
    "requested_address_found": "bool",
    "requested_address": "<address>",
    "requested_address_ip": "<address>_IP",
    "available_storage": "12Go",
    "availability": .99,
    "data_consistency": 1.,
    "avg_network_speed": "24Mo/s",
    "avg_price": "0.000000007btc/Go",
    "current_price": "0.000000006btc/Go",
    "replicated_on": [
        "DID1", "DID2", "DID3"
    ],
    "stored_data_for": [
        "DID4", "DID5", "DID6", "DID7", "DID8", "DID9"
    ],
    "storage_type": "main | replicated",
    "location": "en-gb",
    "bln_address": "0x000000
}
```

#### DID Storage provider reputation

The reputation of a storage provider can be verified by asking a DID for stats
about data storing with this provider.

This can be done by sending a `#DATA` request to 
`did://<address>/storage/stats?<provider>`.

The expected response for such a request is:

```json
{
    "for_provider": "uuid",
    "storage_started_at": "epoch",
    "storage_ended_at": "epoch [OR] -1 (for ongoing)",
    "data_consistency": .95,
    "availability": .9,
    "avg_network_speed": "18Mo/s",
    "avg_price": "0.000000007btc/Go"
}
```

If the asked DID never dealt with this storage provider for storage related
operations, it will return:

```json
{
    "for_provider": "uuid",
    "storage_started_at": "never"
}
```

> [!IMPORTANT]
> This record can also be asked to storage providers used by a specifc storage
> provider for replication.

#### How is data consistency measured?

Data consistency is measured by aggregating the correspondence of a retrieved
dataset with the hash of it made at the last modification.

#### How is availability measured?

Availability is measured by substracting the number of storage requests denied
by the storage provider due to network, TCP, DID, storage, or replicates issues
divided by the number of served requests in a month from 1.

Each storage requester can compute this by dividing the number of denied reqs
with the number of requests made to the storage provider.

### Paying for storage

To benefit from the storage system, a DID server should be configured with a 
BLN wallet. This wallet will be used in micro payments to pay for data monthly.

Every `N` of each month, `N` being the day storage started on a provider, the
DID will perform a micro-transaction using the BLN wallet to this target. If
the micro-transaction is never received by the target at most 72h after the
expected date, the data is deleted.

The amount paid matches the maximum quantity of data stored on the provider and
the price per unit at the moment of the transaction.

> [!IMPORTANT]
> BLN-based micro-payments are meant to be replaced by a Web5 native system.

Disputes are not possible between two DID servers. After 72h, the deletion is
triggered by the verification in the BLN chain for the presence of a 
transaction targeting it from the storage requester BLN address.

### Fresh targets

A fresh target have empty ratings by default. In order to gain ratings, it must 
serve first as a replicated storage provider to get better ratings and 
eventually a trusted source of storage.

> [!NOTE]
> Replicas are not defined as-it in the DID protocol, those are usually chosen
> by other storage providers due to low operational cost, low trust due to a
> small amount of results in the `stored_data_for`.
>
> This highlights a great trust-based system where nodes who store data are
> more trusted (based also on feedbacks from storage requesters), and new ones
> have to prove they are trustworthy.

### Storing some data

For a storage requester to store data on a storage provider, it must find one
that fits its requirements first. After finding one, it must send it a query 
with an occupied storage capacity to get its approval to store some data.

`#DATA did://<address>/storage/occupy`

```jsonc
{
    "reserve_space": "512Mo",
    "storage_method": "raw" // or "kv" for a KV database
    "with_replications": 4
}
```

This request will inform the storage provider of how much space is needed, and
how many replications of stored data are required (a node can request less
replications than what the provider can give, but not more).

The storage provider will have to answer this request by approving or rejecting
this request, if it approves, it have to give the list of nodes used for 
replication to the storage requester for validation of replication and an
approval uuid that refers to this request.

> [!NOTE]
> The approval UUID has a TTL of 20 seconds, during this time, the space
> requested by the requester is reserved for a future acceptation.

```jsonc
{
    "asked_space": "512Mo",
    "with_storage_method": "raw",
    "with_replications": 4,
    "available_space": "6Go",
    "max_replications": 18,
    "approved": true,
    "approval_id": "uuid",
    "bln_address": "0x0000",
    "replicas": [
        {
            "did": "did",
            "ip": "ip",
            "last_checked": "timestamp",
            "alive": "boolean",
            "available_storage": "12Go",
            "availability": .99,
            "data_consistency": .99,
            "avg_network_speed": "12Mo/s",
            "current_price": "0.000000007btc/Go"
        }
        // Replicas list expect each entry to be formatted the same way as
        // lookup results.
    ],
    "stored_data_for": [
        // The same thing applies to this section.
    ]
}
```

The requester will be provided with a list of replicas used by this provider
and the list of DIDs it stored data for.

With this in hands, the requester will ask the 10 latest replicas and clients
for their reputation of the storage provider. If more than 2/3 of the DIDs 
report a bad storage provider (either low stats or never dealt with it for 
storage), the operation is declined.

Otherwise, an acceptation request is sent and a first payment is made to the
BLN address of the storage provider. The acceptation request contains the 
`approval_id` and the stored data encrypted using the DID.

`#DATA did://<address>/storage/create_resource_with_approval_id`

```json
{
    "store_with_approval_id": "uuid",
    "payload": "encrypted_data",

}
```

The storage provider will definitely allocate the space approved through the
approval ID and return `N + 1` identifiers for the resource, `N` being the
number of replicas. Those identifiers will be formatted as URIs encrypted using
the client DID, therefore only the storage provider and the client knows how to
get this data.

The identifier will be formatted as:
`#DATA did://<address or replica address>/storage/<DID>/<resource_id>`

And the response to the `create_resource_with_approval_id` will be, if the 
approval did not time out:

```jsonc
{
    "stored": true,
    id: {
        "encrypted_identifier",
        "encrypted_identifer 1st replica",
        // etc...
    }
}
```

> If it did timeout, the response will be:
> ```json
> {
>     "stored": false,
>     "reason": "timeout"
> }


On the storage provider, data is stored as a JSON object:

```jsonc
{
    "id": "resource_id",
    "owned_by": "client_did",
    "storage_started_at": "epoch",
    "last_payment": "epoch",
    "allocated_size": "512Mo",
    "occupied_size": "58Ko",
    "replicas": [
        // List of replicas
    ],
    "dataset": "encrypted_dataset"
}
```

> [!NOTE]
> The grace period for late payments begins directly, meaning the node have
> 72hrs for paying the first month of storage.

#### Retrieving data

DID nodes can retrieve stored data at anytime, they just have to decrypt
the IDs and make a request to the first URI (the storage provider), if the
storage provider is unavailable, it queries the second URI (first replica), 
etc...

The query that should be sent is:
`#DATA <uri>`

If by any means, the storage provider AND the replicas did not answer, the
storage provider is considered as untrustworthy and data will be moved to
another storage provider.

If the response is successful, the whole stored JSON object is returned to the
client.

> For KV databases, the dataset is the whole database as a JSON object with 
> encrypted keys and values.

#### Modifying data

Sending this request to the storage provider will modify the data saved for
a dataset. However, if the new dataset fills the allocated space for the
client, the query is denied.

`#DATA did://<address>/storage/<DID>/<resource_id>/set`

Request:
```json
{
    "payload": "encrypted_data"
}
```

Response:
```jsonc
{
    "approved": true // or false depending on available space
}
```

> For KV databases, the expected payload is a JSON object with encrypted keys
> and values. If the JSON object is malformed, the storage provider will deny
> the modification. Moreover, using this technique will override the entire
> database.

#### Deleting data

DID nodes can delete data from a storage provider. The deletion has to happen
on the storage provider and not on replicas.

To do this, the node will send a 
`#DATA did://<storage provider addr>/storage/<DID>/<resource_id>/rm` request
to the storage provider and proceed to the last payment.

The storage provider will then be responsible of deleting the dataset on its
replicas.

The answer to this request is a copy of the dataset removed.

#### Data access

If by any means, a DID requests a resource allocated to another DID or try to
make a `rm` request on it. It will be stopped by a basic data access rule:
- Only the owner of a datset can perform set/get/delete actions on it.

Thanks to the advanced DID verification method, storage providers only have to
compare the DID of the requester, and the DID stored on the dataset's metadata.

#### KV database

Storing raw datasets may not be suitable in every situations. Therefore, DID
nodes offers a Key-Value database that can be queried for content and only 
available on resources with the `storage_method` attribute set to `"kv"` on 
creation.

To allow for more granular access, the KV database provides a specific routes
to edit only keys and values.

`#data did://<address>/storage/<DID>/<resource_id>/kv`

Request:
```jsonc
{
    // Every key must be encrypted since all data stored on providers is 
    // encrypted.
    "target": "encryptedKey.encryptedChild.encryptedLeaf",
    "method": "get", // or "set", or "delete"
    // On "set" methods,
    "new_value": "encryptedValue"
}
```

Response:
```jsonc
{
    "for_target": "target",
    "with_method": "method",
    // On "get"
    "value": "JSON",
    // On "set" or "delete":
    "previous_value": "JSON"
}
```

In the case of errors, the database will return:

```jsonc
{
    "for_target": "target",
    "with_method": "method",
    "error": "not_found" // or "not_enough_space"
}
```

### Storage replication

Each target storage provider is replicated to at least 2 other machines to 
ensure storage requesters have backups in case a provider gets offline.

It is up to the storage provider to decide how much replication machines it
is meant to be connected to. The higher the number of replication machines, the
higher the cost of storage.

## Error conditions, timeouts, and failures

> [!NOTE]
> As you see, this protocol favors connection terminations instead of graceful
> degradations. This is due to the tight requirement this protocol has in 
> verifying the identity of everyone. In such a context, there is no way of 
> considering inability to verify an identity not a reason to close a 
> connection, especially when a whole network integrity must be protected.

Any kind of error or failure is accompanied of a title, a kind, a description
and a log.

- `tcp_failure`: Failed to initiate a TCP connection for a specific reason.
- `tcp_connection_closed`: The TCP connection has been closed.
- `did_preflight_ecdh_tampering`: The remote ECDH key has been tampered with,
    indicating either a MITM or a remote fault. But the connection is closed
    due to the major security threat.
- `malformed_request`: Failed to decode/parse the request.
- `did_no_public_session_key`: The target did not provided a public session key.
- `did_no_preflight_response`: The target did not answered the `PREFLIGHT`.
- `did_not_found`: Failed to resolve the IP address of a DID.
- `did_timeout`: Received no response in 30 seconds.
- `did_check_failure`: Failed to correctly authentify a DID.
- `did_no_preflight`: No preflight response have been received.
- `did_dns_not_found`: Failed to find the DID DNS.
- `did_dns_timeout`: Received no response in 30 seconds from the DID DNS.
- `did_lookup_timeout`: Failed if the lookup took more than 30 seconds.

# Roadmap

- [ ] Add support for data fetching and receiving through socket connections
    - [ ] Basic socket connections
    - [ ] Request/response parsing and formatting
        - [ ] `did://` formatting
    - [ ] Support for protocol verbs
    - [ ] `PREFLIGHT` handshake process
        - [ ] Signed session keys exchange
        - [ ] DID verification
    - [ ] Add support for the `DATA` verb to user-defined routes
    - [ ] Encrypted data exchange
- [ ] Add support for neighboring
    - [ ] Full AR tables
        - [ ] Add support for the `#DATA` verb for AR tables
    - [ ] Add support for lookups with `WHERE?`
    - [ ] Add support for storage lookups with `WHERE!`
        - [ ] Add support for filters
    - [ ] DID DNS support with HTTP to DID protocol switching
    - [ ] Neighbor scoring with support for the `#DATA` verb for scoring tables
- [ ] Add support for storage
    - [ ] Add support for the `#DATA` verb for storage providing
    - [ ] Add support for data provider reputation in AR tables for storage
        requesters and locally for the storage provider.
        - [ ] Add metrics for storage performance
    - [ ] Add raw data storage support
    - [ ] Add KV databases support
    - [ ] Add support for encrypted and signed datasets
    - [ ] Add support for data access control
    - [ ] Add support for BLN wallets
        - [ ] Add support for storage payments
        - [ ] Add support for disputes
    - [ ] Add support for storage allocation and approvals
    - [ ] Add support for storage retrieving to storage providers or replicas
    - [ ] Add support for storage operations to the storage provider
- [ ] Add support for errors and timeouts
- [ ] Create library documentation
